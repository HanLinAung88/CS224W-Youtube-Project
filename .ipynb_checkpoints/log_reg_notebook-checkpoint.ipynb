{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from features import topological_features, aggregate_features, get_vars, extract_features\n",
    "import pickle\n",
    "import rolx\n",
    "import numpy as np\n",
    "import utils\n",
    "import random\n",
    "\n",
    "def get_scores(train_pred, train_true, test_pred, test_true):\n",
    "    train_accuracy = np.mean(train_pred == train_true)\n",
    "    print train_accuracy\n",
    "\n",
    "    train_f1 =  precision_recall_fscore_support(train_true, train_pred)\n",
    "    print train_f1[0][1]\n",
    "    print train_f1[1][1]\n",
    "    print train_f1[2][1]\n",
    "    \n",
    "    test_accuracy = np.mean(test_pred == test_true)\n",
    "    print test_accuracy\n",
    "    \n",
    "    test_f1 =  precision_recall_fscore_support(test_true, test_pred)\n",
    "    print test_f1[0][1]\n",
    "    print test_f1[1][1]\n",
    "    print test_f1[2][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vertex Features matrix\n",
      "V is a 3356 by 180 matrix.\n",
      "Node-role matrix is of dimensions 3356 by 5\n",
      "[[0.         0.         0.         0.         0.178456  ]\n",
      " [0.         0.         0.05493915 0.0009054  0.        ]\n",
      " [0.         0.         0.05493915 0.0009054  0.        ]\n",
      " ...\n",
      " [0.         0.         0.05493915 0.0009054  0.        ]\n",
      " [0.         0.         0.05493915 0.0009054  0.        ]\n",
      " [0.         0.         0.05493915 0.0009054  0.        ]]\n",
      "[[1.90000000e+02 2.99760192e-04 2.00000000e+01 ... 2.89920433e-03\n",
      "  2.89920433e-03 2.00000000e+01]\n",
      " [0.00000000e+00 2.99759683e-04 1.00000000e+00 ... 1.67912251e-04\n",
      "  1.67912251e-04 1.00000000e+00]\n",
      " [0.00000000e+00 2.99759683e-04 1.00000000e+00 ... 1.67912251e-04\n",
      "  1.67912251e-04 1.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 2.99759683e-04 1.00000000e+00 ... 1.67912251e-04\n",
      "  1.67912251e-04 1.00000000e+00]\n",
      " [0.00000000e+00 2.99759683e-04 1.00000000e+00 ... 1.67912251e-04\n",
      "  1.67912251e-04 1.00000000e+00]\n",
      " [0.00000000e+00 2.99759683e-04 1.00000000e+00 ... 1.67912251e-04\n",
      "  1.67912251e-04 1.00000000e+00]]\n",
      "[[0.         0.         0.00464583 0.         0.         0.\n",
      "  0.         0.00464505]\n",
      " [0.         0.01143582 0.         0.01037068 0.04039842 0.\n",
      "  0.         0.        ]\n",
      " [0.01366998 0.30420258 0.06550999 0.30512286 0.23939461 0.07769464\n",
      "  0.0776944  0.06550184]\n",
      " [0.         0.00269148 0.00767483 0.00302349 0.         0.\n",
      "  0.         0.00767524]\n",
      " [0.16140212 0.07348006 0.37296117 0.07333278 0.04136768 0.37093409\n",
      "  0.37095024 0.37296296]]\n",
      "Role-feature matrix is of dimensions 5 by 8\n",
      "[[0.         0.         0.00464583 0.         0.         0.\n",
      "  0.         0.00464505]\n",
      " [0.         0.01143582 0.         0.01037068 0.04039842 0.\n",
      "  0.         0.        ]\n",
      " [0.01366998 0.30420258 0.06550999 0.30512286 0.23939461 0.07769464\n",
      "  0.0776944  0.06550184]\n",
      " [0.         0.00269148 0.00767483 0.00302349 0.         0.\n",
      "  0.         0.00767524]\n",
      " [0.16140212 0.07348006 0.37296117 0.07333278 0.04136768 0.37093409\n",
      "  0.37095024 0.37296296]]\n",
      "((3356, 5), (5, 8))\n"
     ]
    }
   ],
   "source": [
    "fname = './dataset/0222/0.txt'\n",
    "fname_extended = './dataset/0222/1.txt'\n",
    "G, dict_to_graph, graph_to_dict = rolx.load_graph_igraph(fname)\n",
    "roles = 5\n",
    "H, R = rolx.extract_rolx_roles(G, roles)\n",
    "print(H.shape, R.shape)\n",
    "H.tolist()\n",
    "\n",
    "# np.save('rolx_features', H)\n",
    "# H = np.load('rolx_features.npy')\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "pos_data = []\n",
    "neg_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "3258\n"
     ]
    }
   ],
   "source": [
    "# extracts data from rolx for features\n",
    "adj_mat = G.get_adjacency()\n",
    "_, video_dict_list, graph_to_dict, neighbors, fields = get_vars(fname, fname_extended)\n",
    "# with open('feature_dict.pkl', 'wb') as f:\n",
    "# \tpickle.dump(feature_dict, f)\n",
    "# feature_dict = pickle.load('feature_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(adj_mat.shape[0]):\n",
    "    H_row = np.array(H[row]).flatten()\n",
    "    for col in range(adj_mat.shape[1]):\n",
    "        H_total = np.array(H[col][0]).flatten() + H_row\n",
    "        # print 'pre concatenated', type(H_total), H_total\n",
    "\n",
    "        # flag for adding into agg and topo features\n",
    "        if True:\n",
    "            local_features = extract_features(video_dict_list, graph_to_dict, neighbors, fields, row, col) \n",
    "            # skip if doesnt exist\n",
    "            if not local_features:\n",
    "                continue\n",
    "\n",
    "            H_total = np.concatenate([H_total, local_features]) \n",
    "            # print 'after concatenated', type(H_total), H_total\n",
    "\n",
    "        if adj_mat[row][col] > 0:\n",
    "            pos_data.append((H_total, adj_mat[row][col]))\n",
    "        else:\n",
    "            neg_data.append((H_total, adj_mat[row][col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10614564, 12)\n"
     ]
    }
   ],
   "source": [
    "# creates positive and negative dataset for more uniform distribution of data\n",
    "X = [pos_data[i][0] for i in range(len(pos_data))]\n",
    "Y = [pos_data[i][1] for i in range(len(pos_data))]\n",
    "\n",
    "# random_indices = sorted(random.sample(range(len(neg_data)), len(X)))\n",
    "X_neg = [neg_data[i][0] for i in range(len(neg_data))] #random_indices]\n",
    "Y_neg = [neg_data[i][1] for i in range(len(neg_data))] #random_indices]\n",
    "\n",
    "X.extend(X_neg)\n",
    "Y.extend(Y_neg)\n",
    "\n",
    "X_array = np.array(X)\n",
    "# X_array = X_array[:, np.r_[:5]]\n",
    "Y_array = np.array(Y)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "# X_array = normalize(X_array[:, :5])\n",
    "print X_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test zero vals 10501538 test one vals 6880\n",
      "10501538 6880\n",
      "(10508418, 12)\n",
      "done splitting\n"
     ]
    }
   ],
   "source": [
    "# runs training by splitting train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, Y_array, test_size=0.01, random_state=42)\n",
    "\n",
    "test_zero_vals = np.argwhere(y_test == 0)\n",
    "test_one_vals = np.argwhere(y_test == 1)\n",
    "print 'test zero vals', len(zero_vals), 'test one vals', len(one_vals)\n",
    "\n",
    "zero_vals = np.argwhere(y_train == 0)\n",
    "one_vals = np.argwhere(y_train == 1)\n",
    "print len(zero_vals), len(one_vals)\n",
    "\n",
    "random_indices = zero_vals[sorted(random.sample(range(len(zero_vals)), len(one_vals)))]\n",
    "\n",
    "random_indices = np.concatenate([random_indices, one_vals]).reshape(-1)\n",
    "\n",
    "print X_train.shape\n",
    "X_train = X_train[random_indices]\n",
    "y_train = y_train[random_indices]\n",
    "\n",
    "print 'done splitting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106079\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "print len(np.argwhere(y_test == 0))\n",
    "print len(np.argwhere(y_test == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest\n",
      "0.9473837209302326\n",
      "0.905\n",
      "0.9997093023255814\n",
      "0.9500000000000001\n",
      "0.8919601303864488\n",
      "0.005808409189423494\n",
      "1.0\n",
      "0.011549732804688847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                              random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'random forest'\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "get_scores(train_predictions, y_train, test_predictions, y_test)\n",
    "np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression\n",
      "0.5011627906976744\n",
      "0.5005829204313611\n",
      "0.998546511627907\n",
      "0.6668608037274316\n",
      "0.004738756052983626\n",
      "0.0006338094787626525\n",
      "1.0\n",
      "0.0012668160375128809\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train, y_train)\n",
    "print 'logistic regression'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "get_scores(train_predictions, y_train, test_predictions, y_test)\n",
    "np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_rbf\n",
      "0.5011627906976744\n",
      "0.5005829204313611\n",
      "0.998546511627907\n",
      "0.6668608037274316\n",
      "0.004738756052983626\n",
      "0.0006338094787626525\n",
      "1.0\n",
      "0.0012668160375128809\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'svm_rbf'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_preds, y_train, test_preds, y_test)\n",
    "# np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm linear\n",
      "0.5011627906976744\n",
      "0.5005829204313611\n",
      "0.998546511627907\n",
      "0.6668608037274316\n",
      "0.004738756052983626\n",
      "0.0006338094787626525\n",
      "1.0\n",
      "0.0012668160375128809\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm.SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'svm linear'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_preds, y_train, test_preds, y_test)\n",
    "# np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished knn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justinxu/.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:268: DeprecationWarning: check_pickle is deprecated in joblib 0.12 and will be removed in 0.13\n",
      "  ' removed in 0.13', DeprecationWarning)\n",
      "/home/justinxu/.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:268: DeprecationWarning: check_pickle is deprecated in joblib 0.12 and will be removed in 0.13\n",
      "  ' removed in 0.13', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9367732558139535\n",
      "0.8889464147035983\n",
      "0.9982558139534884\n",
      "0.9404354374914419\n",
      "0.7972132722853428\n",
      "0.0025961984237366713\n",
      "0.835820895522388\n",
      "0.0051763183435781296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'finished knn'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_preds, y_train, test_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes\n",
      "0.7367732558139535\n",
      "0.750384260682447\n",
      "0.709593023255814\n",
      "0.7294187957567608\n",
      "0.7676031126938367\n",
      "0.0018225264266331861\n",
      "0.6716417910447762\n",
      "0.0036351886258986993\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'naive bayes'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_preds, y_train, test_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
