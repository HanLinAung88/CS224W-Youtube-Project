{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from features import topological_features, aggregate_features, get_vars, extract_features\n",
    "import pickle\n",
    "import rolx\n",
    "import numpy as np\n",
    "import utils\n",
    "import random\n",
    "\n",
    "def get_scores(train_pred, train_true, val_pred, val_true, test_pred, test_true):\n",
    "    train_accuracy = np.mean(train_pred == train_true)\n",
    "    print train_accuracy\n",
    "\n",
    "    train_f1 =  precision_recall_fscore_support(train_true, train_pred)\n",
    "    print train_f1[0][1]\n",
    "    print train_f1[1][1]\n",
    "    print train_f1[2][1]\n",
    "    \n",
    "    val_accuracy = np.mean(val_pred == val_true)\n",
    "    print val_accuracy\n",
    "    \n",
    "    val_f1 =  precision_recall_fscore_support(val_true, val_pred)\n",
    "    print val_f1[0][1]\n",
    "    print val_f1[1][1]\n",
    "    print val_f1[2][1]\n",
    "\n",
    "    test_accuracy = np.mean(test_pred == test_true)\n",
    "    print test_accuracy\n",
    "    \n",
    "    test_f1 =  precision_recall_fscore_support(test_true, test_pred)\n",
    "    print test_f1[0][1]\n",
    "    print test_f1[1][1]\n",
    "    print test_f1[2][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolx(fname, fname_extended, roles=3):\n",
    "    G, dict_to_graph, graph_to_dict = rolx.load_graph_igraph(fname, fname_extended)\n",
    "    H, R = rolx.extract_rolx_roles(G, roles)\n",
    "    print(H.shape, R.shape)\n",
    "    H.tolist()\n",
    "\n",
    "    adj_mat = G.get_adjacency()\n",
    "    _, video_dict_list, graph_to_dict, neighbors, fields = get_vars(fname, fname_extended)\n",
    "    # np.save('rolx_features', H)\n",
    "    # H = np.load('rolx_features.npy')\n",
    "    \n",
    "    return adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields\n",
    "\n",
    "def get_features(adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields, agg_flag=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    pos_data = []\n",
    "    neg_data = []\n",
    "    for row in range(adj_mat.shape[0]):\n",
    "        H_row = np.array(H[row]).flatten()\n",
    "        for col in range(adj_mat.shape[1]):\n",
    "            H_total = np.array(H[col][0]).flatten() + H_row\n",
    "            # print 'pre concatenated', type(H_total), H_total\n",
    "\n",
    "            # flag for adding into agg and topo features\n",
    "            if agg_flag:\n",
    "                local_features = extract_features(video_dict_list, graph_to_dict, neighbors, fields, row, col) \n",
    "                # skip if doesnt exist\n",
    "                if not local_features:\n",
    "                    continue\n",
    "\n",
    "                H_total = np.concatenate([H_total, local_features]) \n",
    "                # print 'after concatenated', type(H_total), H_total\n",
    "\n",
    "            if adj_mat[row][col] > 0:\n",
    "                pos_data.append((H_total, adj_mat[row][col]))\n",
    "            else:\n",
    "                neg_data.append((H_total, adj_mat[row][col]))\n",
    "    \n",
    "    return pos_data, neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vertex Features matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rolx.py:95: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  x_star, residuals, rank, s = lstsq(A, w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V is a 3356 by 485 matrix.\n",
      "Node-role matrix is of dimensions 3356 by 3\n",
      "[[0.00126998 0.23255645 0.0053801 ]\n",
      " [0.         0.23278286 0.00700886]\n",
      " [0.         0.23278286 0.00700886]\n",
      " ...\n",
      " [0.         0.01115651 0.02619573]\n",
      " [0.         0.01115651 0.02619573]\n",
      " [0.         0.01244527 0.02430484]]\n",
      "[[4.52246136e+04 1.42941075e-03 2.10000000e+01 ... 3.28100190e-04\n",
      "  3.28100190e-04 2.10000000e+01]\n",
      " [1.63742690e-01 1.42797251e-03 2.00000000e+01 ... 3.07114183e-04\n",
      "  3.07114183e-04 2.00000000e+01]\n",
      " [1.63742690e-01 1.42797251e-03 2.00000000e+01 ... 3.07114183e-04\n",
      "  3.07114183e-04 2.00000000e+01]\n",
      " ...\n",
      " [0.00000000e+00 2.99759763e-04 4.00000000e+00 ... 1.75086647e-04\n",
      "  1.75086647e-04 4.00000000e+00]\n",
      " [0.00000000e+00 2.99759763e-04 4.00000000e+00 ... 1.75086647e-04\n",
      "  1.75086647e-04 4.00000000e+00]\n",
      " [0.00000000e+00 2.99759790e-04 5.00000000e+00 ... 2.10005872e-04\n",
      "  2.10005872e-04 5.00000000e+00]]\n",
      "[[0.00407925 0.         0.01161068 0.         0.         0.\n",
      "  0.         0.01161172]\n",
      " [0.00833944 0.05134342 0.09566065 0.06935981 0.04848973 0.06949611\n",
      "  0.06949549 0.09566112]\n",
      " [0.03806223 0.11440583 0.06035064 0.10210945 0.11152687 0.07470433\n",
      "  0.07470537 0.0603472 ]]\n",
      "Role-feature matrix is of dimensions 3 by 8\n",
      "[[0.00407925 0.         0.01161068 0.         0.         0.\n",
      "  0.         0.01161172]\n",
      " [0.00833944 0.05134342 0.09566065 0.06935981 0.04848973 0.06949611\n",
      "  0.06949549 0.09566112]\n",
      " [0.03806223 0.11440583 0.06035064 0.10210945 0.11152687 0.07470433\n",
      "  0.07470537 0.0603472 ]]\n",
      "((3356, 3), (3, 8))\n"
     ]
    }
   ],
   "source": [
    "fname = './dataset/0222/0.txt'\n",
    "fname_extended = './dataset/0222/1.txt'\n",
    "\n",
    "adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields = get_rolx(fname, fname_extended)\n",
    "pos_data, neg_data = get_features(adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vertex Features matrix\n",
      "V is a 4330 by 408 matrix.\n",
      "Node-role matrix is of dimensions 4330 by 3\n",
      "[[2.18967024 0.32158417 0.0033941 ]\n",
      " [2.43212908 0.25124375 0.01179091]\n",
      " [1.45787963 0.45599134 0.01184172]\n",
      " ...\n",
      " [0.21869531 0.17029455 0.1291226 ]\n",
      " [0.54564185 0.15036211 0.15531668]\n",
      " [0.27499615 0.19094136 0.13176154]]\n",
      "[[1.27113060e+02 2.38829560e-04 3.30000000e+01 ... 3.89347401e-04\n",
      "  3.89347401e-04 3.30000000e+01]\n",
      " [1.60207092e+02 2.38829612e-04 3.70000000e+01 ... 4.32542313e-04\n",
      "  4.32542313e-04 3.70000000e+01]\n",
      " [1.80684843e+02 2.38829428e-04 2.30000000e+01 ... 2.86518924e-04\n",
      "  2.86518924e-04 2.30000000e+01]\n",
      " ...\n",
      " [1.86289381e+03 4.83376818e-04 1.10000000e+01 ... 2.02291225e-04\n",
      "  2.02291225e-04 1.10000000e+01]\n",
      " [1.26525601e+03 4.83377303e-04 2.00000000e+01 ... 2.92377077e-04\n",
      "  2.92377077e-04 2.00000000e+01]\n",
      " [1.51425951e+02 4.83314162e-04 1.20000000e+01 ... 1.93842442e-04\n",
      "  1.93842442e-04 1.20000000e+01]]\n",
      "[[0.         0.         0.02432228 0.         0.         0.\n",
      "  0.         0.02431904]\n",
      " [0.         0.         0.         0.         0.00325712 0.\n",
      "  0.         0.        ]\n",
      " [0.0369099  0.15426662 0.1296719  0.15969016 0.13119936 0.12139501\n",
      "  0.12138993 0.12968341]]\n",
      "Role-feature matrix is of dimensions 3 by 8\n",
      "[[0.         0.         0.02432228 0.         0.         0.\n",
      "  0.         0.02431904]\n",
      " [0.         0.         0.         0.         0.00325712 0.\n",
      "  0.         0.        ]\n",
      " [0.0369099  0.15426662 0.1296719  0.15969016 0.13119936 0.12139501\n",
      "  0.12138993 0.12968341]]\n",
      "((4330, 3), (3, 8))\n"
     ]
    }
   ],
   "source": [
    "fname_test = './dataset/080327/0.txt'\n",
    "fname_test_extended = './dataset/080327/1.txt'\n",
    "\n",
    "adj_mat_test, H_test, video_dict_list_test, graph_to_dict_test, neighbors_test, fields_test = get_rolx(fname_test, fname_test_extended)\n",
    "pos_data_test, neg_data_test = get_features(adj_mat_test, H_test, video_dict_list_test, graph_to_dict_test, neighbors_test, fields_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_balanced(pos_data, neg_data):\n",
    "    # creates positive and negative dataset for more uniform distribution of data\n",
    "    X = [pos_data[i][0] for i in range(len(pos_data))]\n",
    "    Y = [pos_data[i][1] for i in range(len(pos_data))]\n",
    "\n",
    "    random_indices = sorted(random.sample(range(len(neg_data)), len(X)))\n",
    "    X_neg = [neg_data[i][0] for i in random_indices]\n",
    "    Y_neg = [neg_data[i][1] for i in random_indices]\n",
    "\n",
    "    X.extend(X_neg)\n",
    "    Y.extend(Y_neg)\n",
    "\n",
    "    X_array = np.array(X)\n",
    "    Y_array = np.array(Y)\n",
    "    \n",
    "    print X_array.shape, Y_array.shape\n",
    "    from sklearn.preprocessing import normalize\n",
    "    # change this line to change the number of features\n",
    "    X_array = X_array[:, np.r_[:3]]\n",
    "    print X_array.shape\n",
    "\n",
    "    # runs training by splitting train/test sets\n",
    "    return train_test_split(X_array, Y_array, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(pos_data, neg_data):\n",
    "    # runs training by splitting train/test sets\n",
    "    X = [pos_data[i][0] for i in range(len(pos_data))]\n",
    "    Y = [pos_data[i][1] for i in range(len(pos_data))]\n",
    "\n",
    "    X_neg = [neg_data[i][0] for i in range(len(neg_data))]\n",
    "    Y_neg = [neg_data[i][1] for i in range(len(neg_data))]\n",
    "\n",
    "    X.extend(X_neg)\n",
    "    Y.extend(Y_neg)\n",
    "\n",
    "    X_array = np.array(X)\n",
    "    Y_array = np.array(Y)\n",
    "    X_array = X_array[:, np.r_[:3]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_array, Y_array, test_size=0.01, random_state=42)\n",
    "\n",
    "    test_zero_vals = np.argwhere(y_test == 0)\n",
    "    test_one_vals = np.argwhere(y_test == 1)\n",
    "    print 'test zero vals', len(test_zero_vals), 'test one vals', len(test_one_vals)\n",
    "\n",
    "    zero_vals = np.argwhere(y_train == 0)\n",
    "    one_vals = np.argwhere(y_train == 1)\n",
    "    random_indices = zero_vals[sorted(random.sample(range(len(zero_vals)), len(one_vals)))]\n",
    "    random_indices = np.concatenate([random_indices, one_vals]).reshape(-1)\n",
    "\n",
    "    X_train = X_train[random_indices]\n",
    "    y_train = y_train[random_indices]\n",
    "    print X_train.shape, y_train.shape\n",
    "\n",
    "    train_zero_vals = np.argwhere(y_train == 0)\n",
    "    train_one_vals = np.argwhere(y_train == 1)\n",
    "    print 'train zero vals', len(train_zero_vals), 'train one vals', len(train_one_vals)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = split_data(pos_data, neg_data)\n",
    "_, X_test, _, y_test = split_data(pos_data_test, neg_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                              random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'random forest'\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)\n",
    "np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train, y_train)\n",
    "print 'logistic regression'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)\n",
    "np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'svm_rbf'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)\n",
    "# np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm.SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'svm linear'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)\n",
    "# np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'knn'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'naive bayes'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
