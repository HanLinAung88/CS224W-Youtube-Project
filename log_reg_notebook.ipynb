{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from features import topological_features, aggregate_features, get_vars, extract_features\n",
    "import pickle\n",
    "import rolx\n",
    "import numpy as np\n",
    "import utils\n",
    "import random\n",
    "\n",
    "def get_scores(train_pred, train_true, test_pred, test_true):\n",
    "    train_accuracy = np.mean(train_pred == train_true)\n",
    "    print train_accuracy\n",
    "\n",
    "    train_f1 =  precision_recall_fscore_support(train_true, train_pred)\n",
    "    print train_f1[0][1]\n",
    "    print train_f1[1][1]\n",
    "    print train_f1[2][1]\n",
    "    \n",
    "    test_accuracy = np.mean(test_pred == test_true)\n",
    "    print test_accuracy\n",
    "    \n",
    "    test_f1 =  precision_recall_fscore_support(test_true, test_pred)\n",
    "    print test_f1[0][1]\n",
    "    print test_f1[1][1]\n",
    "    print test_f1[2][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolx(fname, fname_extended, roles=3):\n",
    "    G, dict_to_graph, graph_to_dict = rolx.load_graph_igraph(fname, fname_extended)\n",
    "    H, R = rolx.extract_rolx_roles(G, roles)\n",
    "    print(H.shape, R.shape)\n",
    "    H.tolist()\n",
    "\n",
    "    adj_mat = G.get_adjacency()\n",
    "    _, video_dict_list, graph_to_dict, neighbors, fields = get_vars(fname, fname_extended)\n",
    "    # np.save('rolx_features', H)\n",
    "    # H = np.load('rolx_features.npy')\n",
    "    \n",
    "    return adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields\n",
    "\n",
    "def get_features(adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields, agg_flag=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    pos_data = []\n",
    "    neg_data = []\n",
    "    for row in range(adj_mat.shape[0]):\n",
    "        H_row = np.array(H[row]).flatten()\n",
    "        for col in range(adj_mat.shape[1]):\n",
    "            H_total = np.array(H[col][0]).flatten() + H_row\n",
    "            # print 'pre concatenated', type(H_total), H_total\n",
    "\n",
    "            # flag for adding into agg and topo features\n",
    "            if agg_flag:\n",
    "                local_features = extract_features(video_dict_list, graph_to_dict, neighbors, fields, row, col) \n",
    "                # skip if doesnt exist\n",
    "                if not local_features:\n",
    "                    continue\n",
    "\n",
    "                H_total = np.concatenate([H_total, local_features]) \n",
    "                # print 'after concatenated', type(H_total), H_total\n",
    "\n",
    "            if adj_mat[row][col] > 0:\n",
    "                pos_data.append((H_total, adj_mat[row][col]))\n",
    "            else:\n",
    "                neg_data.append((H_total, adj_mat[row][col]))\n",
    "    \n",
    "    return pos_data, neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vertex Features matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rolx.py:95: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  x_star, residuals, rank, s = lstsq(A, w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V is a 3356 by 485 matrix.\n",
      "Node-role matrix is of dimensions 3356 by 3\n",
      "[[0.00126998 0.23255645 0.0053801 ]\n",
      " [0.         0.23278286 0.00700886]\n",
      " [0.         0.23278286 0.00700886]\n",
      " ...\n",
      " [0.         0.01115651 0.02619573]\n",
      " [0.         0.01115651 0.02619573]\n",
      " [0.         0.01244527 0.02430484]]\n",
      "[[4.52246136e+04 1.42941075e-03 2.10000000e+01 ... 3.28100190e-04\n",
      "  3.28100190e-04 2.10000000e+01]\n",
      " [1.63742690e-01 1.42797251e-03 2.00000000e+01 ... 3.07114183e-04\n",
      "  3.07114183e-04 2.00000000e+01]\n",
      " [1.63742690e-01 1.42797251e-03 2.00000000e+01 ... 3.07114183e-04\n",
      "  3.07114183e-04 2.00000000e+01]\n",
      " ...\n",
      " [0.00000000e+00 2.99759763e-04 4.00000000e+00 ... 1.75086647e-04\n",
      "  1.75086647e-04 4.00000000e+00]\n",
      " [0.00000000e+00 2.99759763e-04 4.00000000e+00 ... 1.75086647e-04\n",
      "  1.75086647e-04 4.00000000e+00]\n",
      " [0.00000000e+00 2.99759790e-04 5.00000000e+00 ... 2.10005872e-04\n",
      "  2.10005872e-04 5.00000000e+00]]\n",
      "[[0.00407925 0.         0.01161068 0.         0.         0.\n",
      "  0.         0.01161172]\n",
      " [0.00833944 0.05134342 0.09566065 0.06935981 0.04848973 0.06949611\n",
      "  0.06949549 0.09566112]\n",
      " [0.03806223 0.11440583 0.06035064 0.10210945 0.11152687 0.07470433\n",
      "  0.07470537 0.0603472 ]]\n",
      "Role-feature matrix is of dimensions 3 by 8\n",
      "[[0.00407925 0.         0.01161068 0.         0.         0.\n",
      "  0.         0.01161172]\n",
      " [0.00833944 0.05134342 0.09566065 0.06935981 0.04848973 0.06949611\n",
      "  0.06949549 0.09566112]\n",
      " [0.03806223 0.11440583 0.06035064 0.10210945 0.11152687 0.07470433\n",
      "  0.07470537 0.0603472 ]]\n",
      "((3356, 3), (3, 8))\n"
     ]
    }
   ],
   "source": [
    "fname = './dataset/0222/0.txt'\n",
    "fname_extended = './dataset/0222/1.txt'\n",
    "\n",
    "adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields = get_rolx(fname, fname_extended)\n",
    "pos_data, neg_data = get_features(adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_test = './dataset/080327/0.txt'\n",
    "fname_test_extended = './dataset/080327/1.txt'\n",
    "\n",
    "adj_mat_test, H_test, video_dict_list_test, graph_to_dict_test, neighbors_test, fields_test = get_rolx(fname_test, fname_test_extended)\n",
    "pos_data_test, neg_data_test = get_features(adj_mat_test, H_test, video_dict_list_test, graph_to_dict_test, neighbors_test, fields_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_balanced(pos_data, neg_data):\n",
    "    # creates positive and negative dataset for more uniform distribution of data\n",
    "    X = [pos_data[i][0] for i in range(len(pos_data))]\n",
    "    Y = [pos_data[i][1] for i in range(len(pos_data))]\n",
    "\n",
    "    random_indices = sorted(random.sample(range(len(neg_data)), len(X)))\n",
    "    X_neg = [neg_data[i][0] for i in random_indices]\n",
    "    Y_neg = [neg_data[i][1] for i in random_indices]\n",
    "\n",
    "    X.extend(X_neg)\n",
    "    Y.extend(Y_neg)\n",
    "\n",
    "    X_array = np.array(X)\n",
    "    Y_array = np.array(Y)\n",
    "    \n",
    "    print X_array.shape, Y_array.shape\n",
    "    from sklearn.preprocessing import normalize\n",
    "    # change this line to change the number of features\n",
    "    X_array = X_array[:, np.r_[:3]]\n",
    "    print X_array.shape\n",
    "\n",
    "    # runs training by splitting train/test sets\n",
    "    return train_test_split(X_array, Y_array, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(pos_data, neg_data):\n",
    "    # runs training by splitting train/test sets\n",
    "    X = [pos_data[i][0] for i in range(len(pos_data))]\n",
    "    Y = [pos_data[i][1] for i in range(len(pos_data))]\n",
    "\n",
    "    X_neg = [neg_data[i][0] for i in range(len(neg_data))]\n",
    "    Y_neg = [neg_data[i][1] for i in range(len(neg_data))]\n",
    "\n",
    "    X.extend(X_neg)\n",
    "    Y.extend(Y_neg)\n",
    "\n",
    "    X_array = np.array(X)\n",
    "    Y_array = np.array(Y)\n",
    "    X_array = X_array[:, np.r_[:3]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_array, Y_array, test_size=0.01, random_state=42)\n",
    "\n",
    "    test_zero_vals = np.argwhere(y_test == 0)\n",
    "    test_one_vals = np.argwhere(y_test == 1)\n",
    "    print 'test zero vals', len(test_zero_vals), 'test one vals', len(test_one_vals)\n",
    "\n",
    "    # zero_vals = np.argwhere(y_train == 0)\n",
    "    one_vals = np.argwhere(y_train == 1)\n",
    "    random_indices = zero_vals[sorted(random.sample(range(len(zero_vals)), len(one_vals)))]\n",
    "    random_indices = np.concatenate([random_indices, one_vals]).reshape(-1)\n",
    "\n",
    "    X_train = X_train[random_indices]\n",
    "    y_train = y_train[random_indices]\n",
    "    print X_train.shape, y_train.shape\n",
    "\n",
    "    train_zero_vals = np.argwhere(y_train == 0)\n",
    "    train_one_vals = np.argwhere(y_train == 1)\n",
    "    print 'train zero vals', len(train_zero_vals), 'train one vals', len(train_one_vals)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140070,) (140070,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-45345f3bfcdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data_balanced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_data_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_data_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-d3bc162b89a2>\u001b[0m in \u001b[0;36msplit_data_balanced\u001b[0;34m(pos_data, neg_data)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# change this line to change the number of features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mX_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mX_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "X_train, _, y_train, _ = split_data_balanced(pos_data, neg_data)\n",
    "_, X_test, _, y_test = split_data(pos_data_test, neg_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest\n",
      "0.7310653580301577\n",
      "0.7945686600395121\n",
      "0.6232750350527636\n",
      "0.6985746188414987\n",
      "0.8369415710436569\n",
      "0.014655172413793103\n",
      "0.6100478468899522\n",
      "0.028622741048378045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                              random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'random forest'\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "get_scores(train_predictions, y_train, test_predictions, y_test)\n",
    "np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression\n",
      "0.5\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.003937972226932715\n",
      "0.003937972226932715\n",
      "1.0\n",
      "0.007845050861454149\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train, y_train)\n",
    "print 'logistic regression'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "get_scores(train_predictions, y_train, test_predictions, y_test)\n",
    "np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_rbf\n",
      "0.5\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.003937972226932715\n",
      "0.003937972226932715\n",
      "1.0\n",
      "0.007845050861454149\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'svm_rbf'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_preds, y_train, test_preds, y_test)\n",
    "# np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm linear\n",
      "0.5\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.003937972226932715\n",
      "0.003937972226932715\n",
      "1.0\n",
      "0.007845050861454149\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm.SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'svm linear'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_preds, y_train, test_preds, y_test)\n",
    "# np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justinxu/.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:268: DeprecationWarning: check_pickle is deprecated in joblib 0.12 and will be removed in 0.13\n",
      "  ' removed in 0.13', DeprecationWarning)\n",
      "/home/justinxu/.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py:268: DeprecationWarning: check_pickle is deprecated in joblib 0.12 and will be removed in 0.13\n",
      "  ' removed in 0.13', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8518067547290483\n",
      "0.7731057134127711\n",
      "0.9958920620864389\n",
      "0.8704701089001409\n",
      "0.5203587511540708\n",
      "0.005290590586667711\n",
      "0.645933014354067\n",
      "0.010495218844748503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'knn'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_preds, y_train, test_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes\n",
      "0.7026664698792217\n",
      "0.7875139586823003\n",
      "0.5551127838043933\n",
      "0.6511996998946746\n",
      "0.8507621577826767\n",
      "0.013868751181995839\n",
      "0.5263157894736842\n",
      "0.027025366992199498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'naive bayes'\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_preds, y_train, test_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
